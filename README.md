# Instagram Scraper та Парсер

Цей проект дозволяє автоматизовано отримувати та аналізувати контент з Instagram, включаючи пости та reels. Скрипт використовує Selenium для входу в Instagram, скрапінгу сторінок та збереження їх HTML, а потім парсить ці файли для витягування URL медіа та описів.

## Зміст

- [Опис проекту](#опис-проекту)
- [Встановлення](#встановлення)
- [Налаштування](#налаштування)
- [Використання](#використання)
  - [Консольний режим](#консольний-режим)
  - [Веб-інтерфейс](#веб-інтерфейс)
  - [Інтеграція з ботом](#інтеграція-з-ботом)
- [Структура проекту](#структура-проекту)
- [Функціональність](#функціональність)
- [Обробка помилок](#обробка-помилок)
- [Експорт даних](#експорт-даних)

## Опис проекту

Instagram Scraper та Парсер виконує наступні функції:
1. Авторизація в Instagram через Selenium
2. Скрапінг сторінок з постами та reels
3. Збереження HTML-контенту сторінок
4. Парсинг збережених HTML для витягування URL медіа та описів
5. Збереження отриманих даних у базу даних SQLite

## Встановлення

### Вимоги

- Python 3.7+
- Chrome WebDriver (відповідно до версії вашого браузера Chrome)

### Кроки встановлення

1. Клонуйте репозиторій:
```bash
git clone <url-репозиторію>
cd instagram
```

2. Створіть та активуйте віртуальне середовище:
```bash
python -m venv .venv
# Для Windows
.venv\Scripts\activate
# Для Linux/Mac
source .venv/bin/activate
```

3. Встановіть залежності:
```bash
pip install -r requirements.txt
```

4. Створіть файл `.env` з вашими обліковими даними (див. розділ "Налаштування")

## Налаштування

1. Створіть файл `.env` у кореневій папці проекту з наступними змінними:
```
INSTAGRAM_USERNAME=ваш_логін
INSTAGRAM_PASSWORD=ваш_пароль
URL_DOPYS=https://www.instagram.com/ваш_акаунт/
URL_REELS=https://www.instagram.com/ваш_акаунт/reels/
```

2. Налаштуйте режим роботи браузера (headless або ні) у файлі `func/f_auch.py`:
```python
HEADLESS = True  # або False для відображення браузера
```

## Використання

### Консольний режим

#### Запуск повного процесу

Для виконання всього процесу (скрапінг та парсинг) запустіть:
```bash
python run.py
```

#### Запуск окремих компонентів

Для запуску тільки скрапінгу:
```bash
python selen.py
```

Для запуску тільки парсингу:
```bash
python parser.py
```

Для перегляду вмісту бази даних:
```bash
python view_db.py
```

### Веб-інтерфейс

Проект має зручний веб-інтерфейс на Flask, який дозволяє керувати скрапінгом та переглядати результати через браузер.

#### Запуск веб-інтерфейсу

Для запуску веб-інтерфейсу виконайте:
```bash
python app.py
```

Після запуску веб-інтерфейс буде доступний за адресою: http://localhost:5000/

#### Можливості веб-інтерфейсу

- **Головна сторінка**: Панель керування скрапінгом та статистика
- **Пости**: Перегляд та фільтрація зібраних постів та reels
- **Експорт**: Експорт даних у форматах JSON та CSV
- **Налаштування**: Конфігурація облікових даних та параметрів скрапінгу

### Інтеграція з ботом

Проект містить модуль для інтеграції з ботом, який може періодично запускати скрапінг та відправляти нові дані на ваш сайт.

#### Налаштування бота

Додайте в файл `.env` наступні змінні:
```
WEBSITE_API_URL=https://your-website.com/api/instagram
API_KEY=ваш_секретний_ключ
UPDATE_INTERVAL=3600
```

#### Запуск бота

Для ручного запуску бота:
```bash
python bot_integration.py
```

Для автоматичного запуску за розкладом можна використовувати Windows Task Scheduler або cron на Linux.

## Структура проекту

### Основні файли

- `run.py` - головний скрипт для запуску всього процесу
- `selen.py` - скрипт для скрапінгу сторінок Instagram
- `parser.py` - скрипт для парсингу HTML та збереження даних
- `view_db.py` - скрипт для перегляду даних у базі

### Допоміжні модулі

- `func/` - директорія з допоміжними функціями
  - `f_auch.py` - функції для авторизації та збереження сторінок
  - `f_time.py` - функції для роботи з часом та затримками

### Веб-інтерфейс та інтеграція

- `app.py` - веб-інтерфейс на Flask для керування скрапером
- `bot_integration.py` - модуль для інтеграції з ботом та автоматичного оновлення сайту
- `improvements.py` - модуль з покращеннями для універсальності проекту

### Шаблони та статичні файли

- `templates/` - директорія з HTML шаблонами для веб-інтерфейсу
  - `base.html` - базовий шаблон з навігацією
  - `index.html` - головна сторінка з панеллю керування
  - `posts.html` - сторінка для перегляду постів
  - `settings.html` - сторінка налаштувань
  - `export.html` - сторінка експорту даних
- `static/` - директорія зі статичними файлами
  - `css/` - CSS стилі
  - `js/` - JavaScript файли
  - `img/` - зображення

### Конфігурація та дані

- `.env` - файл з конфігурацією та обліковими даними
- `requirements.txt` - файл з переліком залежностей
- `instagram_data.db` - база даних SQLite з витягнутими даними
- `README.md` - документація проекту

## Функціональність

### Авторизація в Instagram

Скрипт використовує Selenium для автоматичного входу в Instagram. Він обробляє:
- Прийняття cookie
- Введення логіну та паролю
- Перевірку успішності авторизації

### Скрапінг сторінок

Після авторизації скрипт:
1. Відвідує сторінки з постами та reels
2. Виконує автоматичне прокручування для завантаження контенту
3. Зберігає HTML-код сторінок у файли

### Парсинг даних

Парсер обробляє збережені HTML файли:
1. Витягує URL зображень та відео
2. Витягує описи постів та reels
3. Визначає тип контенту (пост або reel)
4. Зберігає дані в базу даних SQLite

### База даних

Дані зберігаються в таблиці `posts` з наступною структурою:
- `id` - унікальний ідентифікатор
- `post_type` - тип контенту (post або reel)
- `media_url` - URL медіа-файлу
- `description` - опис або підпис
- `timestamp` - часова мітка
- `username` - ім'я користувача
- `is_video` - чи є медіа відео
- `parsed_date` - дата парсингу

## Обробка помилок

Скрипт включає розширене логування та обробку помилок:
- Всі дії логуються у файл `parser.log`
- Обробляються помилки авторизації, скрапінгу та парсингу
- Перевіряється наявність дублікатів перед збереженням у базу даних
- HTML файли автоматично видаляються після успішного парсингу

## Експорт даних

Проект підтримує різні формати експорту даних для подальшого використання та інтеграції з іншими системами.

### Експорт через веб-інтерфейс

Веб-інтерфейс пропонує зручні можливості для експорту:

1. **JSON експорт**
   - Повний експорт усіх даних у форматі JSON
   - Ідеально підходить для інтеграції з веб-сайтами та API

2. **CSV експорт**
   - Експорт даних у форматі CSV
   - Зручний для відкриття в Excel та інших табличних редакторах

### Програмний експорт

Для автоматизованого експорту даних можна використовувати функцію `export_data_to_json` з модуля `improvements.py`:

```python
from improvements import export_data_to_json
import sqlite3

# Підключення до бази даних
conn = sqlite3.connect('instagram_data.db')

# Експорт даних у JSON
export_data_to_json(conn, 'output/instagram_export.json')
```

### Автоматична інтеграція з веб-сайтом

Для автоматичної інтеграції з вашим сайтом використовуйте модуль `bot_integration.py`, який відправляє дані на ваш API ендпойнт:

```python
from bot_integration import upload_to_website, get_new_posts_from_db
import sqlite3
from datetime import datetime, timedelta

# Отримуємо нові пости за останню добу
last_update = datetime.now() - timedelta(days=1)
new_posts = get_new_posts_from_db(last_update)

# Відправляємо на сайт
upload_to_website(new_posts)
```

## Обмеження

- Скрипт може не працювати, якщо Instagram змінить структуру своїх сторінок
- Можливі блокування з боку Instagram при частому використанні
- Для роботи потрібен стабільний інтернет-зв'язок

## Подальший розвиток

- Додавання підтримки для обходу двофакторної автентифікації
- Реалізація проксі для уникнення блокувань
- Розширення функціоналу для збору додаткових даних (коментарі, лайки, тощо)
- Інтеграція з іншими базами даних
