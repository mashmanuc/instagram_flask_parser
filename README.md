# Instagram Scraper та Парсер

Цей проект дозволяє автоматизовано отримувати та аналізувати контент з Instagram, включаючи пости та reels. Скрипт використовує Selenium для входу в Instagram, скрапінгу сторінок та збереження їх HTML, а потім парсить ці файли для витягування URL медіа та описів.

## Зміст

- [Опис проекту](#опис-проекту)
- [Встановлення](#встановлення)
- [Налаштування](#налаштування)
- [Використання](#використання)
- [Структура проекту](#структура-проекту)
- [Функціональність](#функціональність)
- [Обробка помилок](#обробка-помилок)

## Опис проекту

Instagram Scraper та Парсер виконує наступні функції:
1. Авторизація в Instagram через Selenium
2. Скрапінг сторінок з постами та reels
3. Збереження HTML-контенту сторінок
4. Парсинг збережених HTML для витягування URL медіа та описів
5. Збереження отриманих даних у базу даних SQLite

## Встановлення

### Вимоги

- Python 3.7+
- Chrome WebDriver (відповідно до версії вашого браузера Chrome)

### Кроки встановлення

1. Клонуйте репозиторій:
```bash
git clone <url-репозиторію>
cd instagram
```

2. Створіть та активуйте віртуальне середовище:
```bash
python -m venv .venv
# Для Windows
.venv\Scripts\activate
# Для Linux/Mac
source .venv/bin/activate
```

3. Встановіть залежності:
```bash
pip install -r requirements.txt
```

4. Створіть файл `.env` з вашими обліковими даними (див. розділ "Налаштування")

## Налаштування

1. Створіть файл `.env` у кореневій папці проекту з наступними змінними:
```
INSTAGRAM_USERNAME=ваш_логін
INSTAGRAM_PASSWORD=ваш_пароль
URL_DOPYS=https://www.instagram.com/ваш_акаунт/
URL_REELS=https://www.instagram.com/ваш_акаунт/reels/
```

2. Налаштуйте режим роботи браузера (headless або ні) у файлі `func/f_auch.py`:
```python
HEADLESS = True  # або False для відображення браузера
```

## Використання

### Запуск повного процесу

Для виконання всього процесу (скрапінг та парсинг) запустіть:
```bash
python run.py
```

### Запуск окремих компонентів

Для запуску тільки скрапінгу:
```bash
python selen.py
```

Для запуску тільки парсингу:
```bash
python parser.py
```

Для перегляду вмісту бази даних:
```bash
python view_db.py
```

## Структура проекту

- `run.py` - головний скрипт для запуску всього процесу
- `selen.py` - скрипт для скрапінгу сторінок Instagram
- `parser.py` - скрипт для парсингу HTML та збереження даних
- `view_db.py` - скрипт для перегляду даних у базі
- `func/` - директорія з допоміжними функціями
  - `f_auch.py` - функції для авторизації та збереження сторінок
  - `f_time.py` - функції для роботи з часом та затримками
- `.env` - файл з конфігурацією та обліковими даними
- `instagram_data.db` - база даних SQLite з витягнутими даними

## Функціональність

### Авторизація в Instagram

Скрипт використовує Selenium для автоматичного входу в Instagram. Він обробляє:
- Прийняття cookie
- Введення логіну та паролю
- Перевірку успішності авторизації

### Скрапінг сторінок

Після авторизації скрипт:
1. Відвідує сторінки з постами та reels
2. Виконує автоматичне прокручування для завантаження контенту
3. Зберігає HTML-код сторінок у файли

### Парсинг даних

Парсер обробляє збережені HTML файли:
1. Витягує URL зображень та відео
2. Витягує описи постів та reels
3. Визначає тип контенту (пост або reel)
4. Зберігає дані в базу даних SQLite

### База даних

Дані зберігаються в таблиці `posts` з наступною структурою:
- `id` - унікальний ідентифікатор
- `post_type` - тип контенту (post або reel)
- `media_url` - URL медіа-файлу
- `description` - опис або підпис
- `timestamp` - часова мітка
- `username` - ім'я користувача
- `is_video` - чи є медіа відео
- `parsed_date` - дата парсингу

## Обробка помилок

Скрипт включає розширене логування та обробку помилок:
- Всі дії логуються у файл `parser.log`
- Обробляються помилки авторизації, скрапінгу та парсингу
- Перевіряється наявність дублікатів перед збереженням у базу даних
- HTML файли автоматично видаляються після успішного парсингу

## Обмеження

- Скрипт може не працювати, якщо Instagram змінить структуру своїх сторінок
- Можливі блокування з боку Instagram при частому використанні
- Для роботи потрібен стабільний інтернет-зв'язок

## Подальший розвиток

- Додавання підтримки для обходу двофакторної автентифікації
- Реалізація проксі для уникнення блокувань
- Розширення функціоналу для збору додаткових даних (коментарі, лайки, тощо)
- Інтеграція з іншими базами даних
